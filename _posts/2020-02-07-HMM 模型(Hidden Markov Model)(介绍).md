使用HMM模型的问题有两类特征：

1.问题是基于序列的，比如时间序列，词序列

2.问题中有两类数据，一类序列数据是可以观测到的，即观测序列；而另一类是不能观测到的，即隐藏状态序列，简称状态序列 （例如词性标注中的词序列与词性序列，语音识别中发出的声音和想要表达的内容）。

### HMM模型的定义

对于HMM模型，首先我们假设𝑄是所有可能的隐藏状态的集合，𝑉是所有可能的观测状态的集合，即：

$$ Q = \{q_1,q_2,...,q_N\}, \; V =\{v_1,v_2,...v_M\} $$

其中，𝑁是可能的隐藏状态数，𝑀是所有的可能的观察状态数。

对于一个长度为𝑇的序列，𝐼对应的状态序列, 𝑂是对应的观察序列，即：

$$ I = \{i_1,i_2,...,i_T\}, \; O =\{o_1,o_2,...o_T\} $$

其中，任意一个隐藏状态 $i_t \in  𝑄$ ,任意一个观察状态 $o_t \in  𝑉$

HMM模型做了两个很重要的假设如下：

1.齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态，。当然这样假设有点极端，因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。但是这样假设的好处就是模型简单，便于求解。如果在时刻$i_t$的隐藏状态是$i_t=q_i$,在时刻t+1的隐藏状态是$i_{t+1}=q_j$, 则从时刻t到时刻t+1的HMM状态转移概率$a_{ij}$可以表示为：

$$a_{ij} = P(i_{t+1} = q_j | i_t= q_i)$$

这样a_{ij}可以组成马尔科夫链的状态转移矩阵𝐴:

$$A=\Big [a_{ij}\Big ]_{N \times N}$$

2.观测独立性假设。即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设。如果在时刻$i_t$的隐藏状态是$i_t = q_i$, 而对应的观察状态为$o_t = v_k$, 则该时刻观察状态$v_k$在隐藏状态$q_j$下生成的概率为𝑏𝑗(𝑘),满足：

$$b_j(k) = P(o_t = v_k | i_t= q_j)$$

这样𝑏𝑗(𝑘)可以组成观测状态生成的概率矩阵𝐵:

$$B = \Big [b_j(k) \Big ]_{N \times M}$$

除此之外，我们需要一组在时刻t=1的隐藏状态概率分布$\Pi$:

$$\Pi = \Big [ \pi(i)\Big ]_N \; 其中 \;\pi(i) = P(i_1 = q_i)$$

　一个HMM模型，可以由隐藏状态初始概率分布\Pi, 状态转移概率矩阵𝐴和观测状态概率矩阵𝐵决定。\Pi,𝐴决定状态序列，𝐵决定观测序列。因此，HMM模型可以由一个三元组𝜆表示如下：

$$\lambda = (A, B, \Pi)$$



### HMM模型一共有三个经典的问题需要解决:

1.评估观察序列概率。即给定模型$\lambda = (A, B, \Pi)$和观测序列$O =\{o_1,o_2,...o_T\}$，
计算在模型$𝜆$下观测序列$𝑂$出现的概率$P(O|λ)$。这个问题的求解需要用到前向后向算法。

2.模型参数学习问题。即给定观测序列$O =\{o_1,o_2,...o_T\}$，估计模型$\lambda = (A, B, \Pi)$的参数，
使该模型下观测序列的条件概率$P(O|λ)$最大。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法。

3.预测问题，也称为解码问题。即给定模型$\lambda = (A, B, \Pi)$和观测序列$O =\{o_1,o_2,...o_T\}$，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法，我们在这个系列的第四篇会详细讲解。这个问题是HMM模型三个问题中复杂度居中的算法。


1.https://www.cnblogs.com/pinard/p/6955871.html (刘建平的微博）

2.https://www.bilibili.com/video/av32471608?p=6  (白板机器学习推导)
